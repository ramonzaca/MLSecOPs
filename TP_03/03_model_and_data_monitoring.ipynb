{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ramonzaca/MLSecOPs/blob/main/TP_03/03_model_and_data_monitoring.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**How's the model performing? - Practice 3**\n",
    "\n",
    "*Now that you have a model, you want to make sure that it's performing well and that the data used to train it is still relevant. This is where model and data monitoring comes into play.*\n",
    "\n",
    "*To do so, you'll use the Evidently library to monitor the model's performance and the data's drift.*\n",
    "\n",
    "*First, let's learn the basics of how to use Evidently to monitor your model and data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install Evidently if it's not already installed\n",
    "try:\n",
    "    import evidently\n",
    "except ImportError:\n",
    "    !pip install git+https://github.com/evidentlyai/evidently.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "from sklearn import datasets\n",
    "from evidently.report import Report\n",
    "from evidently.metric_preset import (\n",
    "    DataDriftPreset,\n",
    ")\n",
    "from evidently.metrics import (\n",
    "    ColumnSummaryMetric,\n",
    "    ColumnQuantileMetric,\n",
    "    ColumnDriftMetric,\n",
    ")\n",
    "from evidently.test_suite import TestSuite\n",
    "from evidently.test_preset import (\n",
    "    DataStabilityTestPreset,\n",
    "    NoTargetPerformanceTestPreset,\n",
    "    RegressionTestPreset,\n",
    ")\n",
    "from evidently.tests import (\n",
    "    TestNumberOfColumnsWithMissingValues,\n",
    "    TestNumberOfRowsWithMissingValues,\n",
    "    TestNumberOfConstantColumns,\n",
    "    TestNumberOfDuplicatedRows,\n",
    "    TestNumberOfDuplicatedColumns,\n",
    "    TestColumnsType,\n",
    "    TestNumberOfDriftedColumns,\n",
    "    TestColumnDrift,\n",
    "    TestMeanInNSigmas,\n",
    "    TestShareOfOutRangeValues,\n",
    ")\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this step, we load the California Housing dataset, prepare it for analysis, and train a Random Forest model. We split the data into reference (training) and current (testing) sets, which is crucial for drift detection.*\n",
    "\n",
    "*In this context, the reference dataset is the training data used to train the model, while the current dataset is the data used to test the model, simulating real-world data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation\n",
    "print(\"1. Data Preparation\")\n",
    "\n",
    "# Load California Housing dataset\n",
    "data = datasets.fetch_california_housing(as_frame=True)\n",
    "housing_data = data.frame\n",
    "housing_data.rename(columns={\"MedHouseVal\": \"target\"}, inplace=True)\n",
    "\n",
    "# Split the data into training (reference) and testing (current) sets\n",
    "\n",
    "reference, current = train_test_split(housing_data, test_size=0.5, random_state=42)\n",
    "\n",
    "# Train a Random Forest model\n",
    "features = [col for col in housing_data.columns if col != \"target\"]\n",
    "rf_model = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_model.fit(reference[features], reference[\"target\"])\n",
    "\n",
    "# Add predictions to both reference and current datasets\n",
    "reference[\"prediction\"] = rf_model.predict(reference[features])\n",
    "current[\"prediction\"] = rf_model.predict(current[features])\n",
    "\n",
    "print(\"Data prepared and model trained successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here, we create a basic data drift report using Evidently's DataDriftPreset. This gives us an initial overview of potential data drift between our reference and current datasets.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Basic Report Generation\n",
    "print(\"\\n2. Basic Report Generation\")\n",
    "\n",
    "# Create a basic data drift report\n",
    "basic_report = Report(metrics=[DataDriftPreset()])\n",
    "basic_report.run(reference_data=reference, current_data=current)\n",
    "print(\"Basic data drift report generated. Use basic_report.show() to display it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*We now create a more focused report with custom metrics. This demonstrates how to analyze specific aspects of your data, such as summary statistics and drift for individual columns. (In this case, we're looking at the 'AveRooms' column.)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Custom Metric Reports\n",
    "print(\"\\n3. Custom Metric Reports\")\n",
    "\n",
    "# Create a report with custom metrics\n",
    "custom_report = Report(\n",
    "    metrics=[\n",
    "        ColumnSummaryMetric(column_name=\"AveRooms\"),\n",
    "        ColumnQuantileMetric(column_name=\"AveRooms\", quantile=0.25),\n",
    "        ColumnDriftMetric(column_name=\"AveRooms\"),\n",
    "    ]\n",
    ")\n",
    "custom_report.run(reference_data=reference, current_data=current)\n",
    "print(\"Custom metric report generated. Use custom_report.show() to display it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*This section introduces a function to generate metrics for multiple columns efficiently. We then use this function to create a report focusing on quantile metrics for selected columns.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Generate Column Metrics\n",
    "print(\"\\n4. Generate Column Metrics\")\n",
    "\n",
    "\n",
    "def generate_column_metrics(metric_class, parameters):\n",
    "    # Generate metrics for all numeric columns\n",
    "    columns = reference.select_dtypes(include=[np.number]).columns.tolist()\n",
    "    return [metric_class(column_name=col, **parameters) for col in columns]\n",
    "\n",
    "\n",
    "# Generate metrics for multiple columns\n",
    "multi_column_report = Report(\n",
    "    metrics=generate_column_metrics(\n",
    "        ColumnQuantileMetric,\n",
    "        parameters={\"quantile\": 0.25},\n",
    "    )\n",
    ")\n",
    "multi_column_report.run(reference_data=reference, current_data=current)\n",
    "print(\"Multi-column report generated. Use multi_column_report.show() to display it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "multi_column_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Here, we combine various metrics to create a comprehensive report. This includes column summaries, quantile metrics for numeric columns, and overall data drift analysis.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. Comprehensive Report\n",
    "print(\"\\n5. Comprehensive Report\")\n",
    "\n",
    "# Create a comprehensive report with various metrics\n",
    "comprehensive_report = Report(\n",
    "    metrics=[\n",
    "        ColumnSummaryMetric(column_name=\"AveRooms\"),\n",
    "        *generate_column_metrics(ColumnQuantileMetric, parameters={\"quantile\": 0.25}),\n",
    "        DataDriftPreset(),\n",
    "    ]\n",
    ")\n",
    "comprehensive_report.run(reference_data=reference, current_data=current)\n",
    "print(\"Comprehensive report generated. Use comprehensive_report.show() to display it.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensive_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now that we have generated reports, we can export them as HTML or JSON files, allowing for easy sharing and integration with other tools.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. Exporting Reports\n",
    "print(\"\\n6. Exporting Reports\")\n",
    "\n",
    "# Uncomment these lines to save the report\n",
    "# comprehensive_report.save_html(\"report.html\")\n",
    "# comprehensive_report.save_json(\"report.json\")\n",
    "print(\"Report can be exported as HTML or JSON.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now that we have our reports and metrics; we can shift focus to test suites, starting with a basic suite that checks for common data quality issues like missing values, constant columns, and duplicates.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. Test Suites\n",
    "print(\"\\n7. Test Suites\")\n",
    "\n",
    "# Create a basic test suite\n",
    "basic_suite = TestSuite(\n",
    "    tests=[\n",
    "        TestNumberOfColumnsWithMissingValues(),\n",
    "        TestNumberOfRowsWithMissingValues(),\n",
    "        TestNumberOfConstantColumns(),\n",
    "        TestNumberOfDuplicatedRows(),\n",
    "        TestNumberOfDuplicatedColumns(),\n",
    "        TestColumnsType(),\n",
    "        TestNumberOfDriftedColumns(),\n",
    "    ]\n",
    ")\n",
    "basic_suite.run(reference_data=reference, current_data=current)\n",
    "print(\"Basic test suite executed. Use basic_suite.show() to display results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "basic_suite.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Same as with the reports, we can use preset test suites to quickly check for common issues.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 8. Preset Test Suites\n",
    "print(\"\\n8. Preset Test Suites\")\n",
    "\n",
    "# Use a preset test suite\n",
    "preset_suite = TestSuite(tests=[NoTargetPerformanceTestPreset()])\n",
    "preset_suite.run(reference_data=reference, current_data=current)\n",
    "print(\"Preset test suite executed. Use preset_suite.show() to display results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preset_suite.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*And we can also create custom test suites to check for specific conditions or drift in our data.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 9. Custom Test Suites\n",
    "print(\"\\n9. Custom Test Suites\")\n",
    "\n",
    "# Create a custom test suite\n",
    "custom_suite = TestSuite(\n",
    "    tests=[\n",
    "        TestColumnDrift(\"Population\"),\n",
    "        TestMeanInNSigmas(\"HouseAge\"),\n",
    "        NoTargetPerformanceTestPreset(columns=[\"AveRooms\", \"AveBedrms\", \"AveOccup\"]),\n",
    "    ]\n",
    ")\n",
    "custom_suite.run(reference_data=reference, current_data=current)\n",
    "print(\"Custom test suite executed. Use custom_suite.show() to display results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "custom_suite.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Once we have our test suites, we can create a comprehensive suite that checks for all the issues we care about.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 10. Comprehensive Test Suite\n",
    "print(\"\\n10. Comprehensive Test Suite\")\n",
    "\n",
    "# Create a comprehensive test suite\n",
    "comprehensive_suite = TestSuite(\n",
    "    tests=[\n",
    "        TestNumberOfColumnsWithMissingValues(),\n",
    "        TestNumberOfRowsWithMissingValues(),\n",
    "        TestNumberOfConstantColumns(),\n",
    "        TestNumberOfDuplicatedRows(),\n",
    "        TestNumberOfDuplicatedColumns(),\n",
    "        TestColumnsType(),\n",
    "        TestNumberOfDriftedColumns(),\n",
    "        TestColumnDrift(\"Population\"),\n",
    "        TestShareOfOutRangeValues(\"Population\"),\n",
    "        DataStabilityTestPreset(),\n",
    "        RegressionTestPreset(),\n",
    "    ]\n",
    ")\n",
    "comprehensive_suite.run(reference_data=reference, current_data=current)\n",
    "print(\n",
    "    \"Comprehensive test suite executed. Use comprehensive_suite.show() to display results.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comprehensive_suite.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally, we can export the test suite results*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 11. Exporting Test Suites\n",
    "print(\"\\n11. Exporting Test Suites\")\n",
    "\n",
    "# Uncomment these lines to save the test suite results\n",
    "# comprehensive_suite.save_html('test_suite.html')\n",
    "# comprehensive_suite.save_json('test_suite.json')\n",
    "\n",
    "print(\"Test suite results can be exported as HTML or JSON.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
