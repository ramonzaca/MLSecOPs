{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ramonzaca/MLSecOPs/blob/main/TP_04/04_bias_and_explainability.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**What's the model doing? - Practice 4**\n",
    "\n",
    "*The model is working and we are keeping track of the data and the model's performance.*\n",
    "\n",
    "*Now, we want to understand what the model is doing. Does it have biases? Are the predictions fair? We want to understand the model's predictions and the model's behavior.*\n",
    "\n",
    "*We will use [SHAP](https://shap.readthedocs.io/en/latest/index.html) to do so.*\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![ShapValues](https://shap.readthedocs.io/en/latest/_images/shap_header.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, let's install Shap if it's not already installed\n",
    "try:\n",
    "    import shap\n",
    "except ImportError:\n",
    "    !pip install shap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warnings for cleaner output\n",
    "import warnings\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "warnings.simplefilter(\"ignore\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import time\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import scipy as sp\n",
    "import shap\n",
    "from sklearn import linear_model, tree\n",
    "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*In this first part we will use the diabetes dataset to create some models and then explain those models' predictions.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Data Preparation\n",
    "print(\"1. Data Preparation\")\n",
    "\n",
    "X, y = shap.datasets.diabetes()\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=0)\n",
    "\n",
    "# rather than use the whole training set to estimate expected values, we summarize with\n",
    "# a set of weighted kmeans, each weighted by the number of points they represent.\n",
    "X_train_summary = shap.kmeans(X_train, 10)\n",
    "\n",
    "\n",
    "def print_accuracy(f):\n",
    "    print(\n",
    "        f\"Root mean squared test error = {np.sqrt(np.mean((f(X_test) - y_test) ** 2))}\"\n",
    "    )\n",
    "    time.sleep(0.5)  # to let the print get out before any progress bars\n",
    "\n",
    "\n",
    "shap.initjs()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Before going further, let's compute a hierarchical clustering of the input features*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "partition_tree = shap.utils.partition_tree(X)\n",
    "plt.figure(figsize=(15, 6))\n",
    "sp.cluster.hierarchy.dendrogram(partition_tree, labels=X.columns)\n",
    "plt.title(\"Hierarchical Clustering Dendrogram\")\n",
    "plt.xlabel(\"feature\")\n",
    "plt.ylabel(\"distance\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's begin with a rather basic model, a linear regression model.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lin_regr = linear_model.LinearRegression()\n",
    "lin_regr.fit(X_train, y_train)\n",
    "\n",
    "print_accuracy(lin_regr.predict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain a single prediction from the test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First create the explainer object (Notice that we use the summary of the training set as the background dataset)\n",
    "ex = shap.KernelExplainer(lin_regr.predict, X_train_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's explain the prediction of the first sample in the test set\n",
    "shap_values = ex.shap_values(X_test.iloc[0, :])\n",
    "shap.force_plot(ex.expected_value, shap_values, X_test.iloc[0, :])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Explain all the predictions in the test set*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap_values = ex.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the dependence of the bmi feature with regards to the age\n",
    "shap.dependence_plot(\"bmi\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's see the force plot of all the samples in the test set\n",
    "shap.force_plot(ex.expected_value, shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's try a decision tree regressor*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dtree = tree.DecisionTreeRegressor(min_samples_split=20)\n",
    "dtree.fit(X_train, y_train)\n",
    "print_accuracy(dtree.predict)\n",
    "\n",
    "# explain all the predictions in the test set\n",
    "ex = shap.TreeExplainer(dtree)\n",
    "shap_values = ex.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"bmi\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(ex.expected_value, shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Let's try a RandomForest*\n",
    "\n",
    "*In this case, we will use the fast `TreeExplainer` implementation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rforest = RandomForestRegressor(\n",
    "    n_estimators=1000, max_depth=None, min_samples_split=2, random_state=0\n",
    ")\n",
    "rforest.fit(X_train, y_train)\n",
    "print_accuracy(rforest.predict)\n",
    "\n",
    "# explain all the predictions in the test set\n",
    "explainer = shap.TreeExplainer(rforest)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"bmi\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Finally, let's train a Neural network*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nn = MLPRegressor(solver=\"lbfgs\", alpha=1e-1, hidden_layer_sizes=(5, 2), random_state=0)\n",
    "nn.fit(X_train, y_train)\n",
    "print_accuracy(nn.predict)\n",
    "\n",
    "# explain all the predictions in the test set\n",
    "explainer = shap.KernelExplainer(nn.predict, X_train_summary)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.summary_plot(shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.dependence_plot(\"bmi\", shap_values, X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.force_plot(explainer.expected_value, shap_values, X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*How representative is each explainer with regards to a generic one?*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "instance = X[0:1]\n",
    "\n",
    "\n",
    "# build a masker from partition tree\n",
    "masker = shap.maskers.Partition(X, clustering=partition_tree)\n",
    "\n",
    "# build explainer objects\n",
    "raw_explainer = shap.PartitionExplainer(rforest.predict, X)\n",
    "masker_explainer = shap.PartitionExplainer(rforest.predict, masker)\n",
    "\n",
    "# compute SHAP values\n",
    "raw_shap_values = raw_explainer(instance)\n",
    "masker_shap_values = masker_explainer(instance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# comparison the masker and the original data sizes\n",
    "print(f\"X size: {sys.getsizeof(X)/1024:.2f} kB\")\n",
    "print(f\"masker size: {sys.getsizeof(masker)} B\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Compare to Tree SHAP*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_explainer = shap.TreeExplainer(rforest, X)\n",
    "tree_shap_values = tree_explainer(instance)\n",
    "\n",
    "plt.figure(figsize=(15, 6))\n",
    "plt.plot(tree_shap_values[0].values, label=\"Tree SHAP\")\n",
    "plt.plot(masker_shap_values[0].values, \"g--\", label=\"Partition SHAP\")\n",
    "plt.plot(raw_shap_values[0].values, \"r--\", label=\"Raw SHAP\")\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Partition SHAP values using a partition tree are nice estimation of SHAP values. The partition tree is a good way to reduce the number of input features and speed up the computation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Another way to plot the explaination of the instance*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "shap.plots.waterfall(masker_shap_values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Now let's try to do a classification analysis. For that we will use the diabetes dataset but classifying whether the person has diabetes or not based on the mean of the dataset.*\n",
    "\n",
    "*(This is for educational purposes only. This is not a good way to do a diabetes classification :)*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train_classification = (y_train < y_train.mean()).astype(int)\n",
    "y_test_classification = (y_test < y_train.mean()).astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the model\n",
    "rf_clf = RandomForestClassifier(max_features=2, n_estimators=50, bootstrap=True)\n",
    "\n",
    "rf_clf.fit(X_train, y_train_classification)\n",
    "\n",
    "# Make prediction on the testing data\n",
    "y_pred = rf_clf.predict(X_test)\n",
    "\n",
    "# Classification Report\n",
    "print(classification_report(y_pred, y_test_classification))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# explain all the predictions in the test set for the negative class\n",
    "# (Notice the time difference between sampling and using the full dataset)\n",
    "explainer = shap.KernelExplainer(rf_clf.predict_proba, X_train)\n",
    "shap_values = explainer.shap_values(X_test)\n",
    "shap.force_plot(explainer.expected_value[0], shap_values[..., 0], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Explain all the predictions in the test set for the positive class. Notice something weird?\n",
    "shap.force_plot(explainer.expected_value[1], shap_values[..., 1], X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
